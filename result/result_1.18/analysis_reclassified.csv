Model Family,Model Name,Original Type,New Category,Overlap Ratio,Notes

# QWEN MODELS
Qwen,Bakanayatsu/runner-Qwen2.5-0.5B-v1,？,Category 1: TRL/Generic SFT,0.0001,Unknown but likely TRL
Qwen,petkopetkov/Qwen2.5-0.5B-song-lyrics-generation,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.0389,TRL-based
Qwen,shb2024/Qwen-0.5B-AMR,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.0719,TRL-based
Qwen,JonhTheTrueKingoftheNorth/Qwen2.5-0.5b,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.1428,TRL-based
Qwen,EdBerg/outputs_qwen_100,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.1485,TRL-based
Qwen,josang1204/Qweb2.5-FT-CSY,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.1559,TRL-based
Qwen,cavendishlabs/sftd_model_c03f6a40-3b64-4f9b-b5f3-dd704171c333,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.2024,TRL-based
Qwen,EdBerg/outputs_qwen_232,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.2617,TRL-based
Qwen,JayHyeon/Qwen2.5-0.5B_ultrainteract_sft_2e-5_1ep,domain-specific(? (TRL,Category 1: TRL/Generic SFT,0.2936,TRL-based

Qwen,alamios/Qwemma-3-0.5B,Hybrid / tokenizer(vocab),Category 2: Architecture/Vocab Changes,0.004,Tokenizer modification
Qwen,flyyufelix/Qwen-2.5-7B-Simple-RL,size,Category 2: Architecture/Vocab Changes,0.0059,Size mismatch (0.5B vs 7B)
Qwen,alamios/Qwenstral-Small-3.1-0.5B,Hybrid / tokenizer(vocab),Category 2: Architecture/Vocab Changes,0.0125,Tokenizer modification
Qwen,rdsm/QwenPhi-4-0.5b-Draft,Hybrid / tokenizer(vocab),Category 2: Architecture/Vocab Changes,0.0161,Tokenizer modification
Qwen,Vikhrmodels/salt-qwen2.5-0.5b-tts,extensions that introduce audio token vocabularies,Category 2: Architecture/Vocab Changes,0.0162,Audio tokens added
Qwen,sail/Sailor2-1B-Pre,Hybrid "expanded from Qwen2.5 base" + model expansion,Category 2: Architecture/Vocab Changes,0.0425,Model expansion 0.5B→1B
Qwen,sail/Sailor2-1B,Hybrid "expanded from Qwen2.5 base" + model expansion,Category 2: Architecture/Vocab Changes,0.0435,Model expansion 0.5B→1B
Qwen,DiamondGotCat/Prism-0.2,Multimodal 扩展 / Hybrid,Category 2: Architecture/Vocab Changes,0.132,Multimodal extension

Qwen,alothomas/Qwen2.5-0.5B-PRM-RAD-balanced-150k,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0231,Process reward model
Qwen,alothomas/Qwen2.5-0.5B-PRM-RAD-balanced-V3,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0299,Process reward model
Qwen,tcapelle/toxicity-scorer-qwen-ct2,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0392,Toxicity scorer
Qwen,alothomas/Qwen2.5-0.5B-PRM-RAD-balanced-V4,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0451,Process reward model
Qwen,ytzfhqs/Qwen2.5-med-book-main-classification,Classifier（文本分类）+ 强领域化（医学教材/OCR）,Category 3: Task Repurposing (Classifier/Scorer),0.0504,Medical text classifier
Qwen,tcapelle/toxicity-scorer-qwen,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0724,Toxicity scorer
Qwen,Aleksandr505/phishing-text-classifier-qwen-2.5-0.5B,Reward/Scorer/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.0995,Phishing classifier
Qwen,jhu-clsp/rank1-0.5b,Scorer/Ranker/Classifier,Category 3: Task Repurposing (Classifier/Scorer),0.1874,Ranker model

Qwen,p1atdev/qwen2.5-0.5b-grpo-math-01,RL,Category 4: RL/Preference Optimization,0.0082,GRPO for math
Qwen,JayHyeon/QWEN_toy-DPOP_5e-5-100ep_0alp_5lam,Preference Optimization（DPO / DPOP）RL,Category 4: RL/Preference Optimization,0.1572,DPO/DPOP
Qwen,robertvacareanu/instruct-rl-Qwen2.5-0.5B-databricks-dolly-15k,RL,Category 4: RL/Preference Optimization,0.204,RL-based instruction tuning

Qwen,jiangchengchengNLP/qwen_0.5B_instruct_law_summarize,Strong field localization（SFT + Contrastive ranking）,Category 5: Heavy Domain Specialization,0.0186,Legal domain + contrastive
Qwen,Qwen/Qwen2.5-Coder-0.5B,Code-specialized / Continued pretraining,Category 5: Heavy Domain Specialization,0.1354,Code specialization + continued PT
Qwen,thundax/Qwen2.5-0.5B-Sign,强任务化（text → Chinese Sign 表达）/ 结构化输出类 SFT,Category 5: Heavy Domain Specialization,0.2739,Chinese Sign Language

Qwen,minchyeom/bot3-0.5B,Instruct SFT,Category 6: Standard Instruction Tuning,0.2509,Standard instruct
Qwen,Silin1590/Qwen-0d5B-Int-Soc-CoA,Instruct / Post-training,Category 6: Standard Instruction Tuning,0.2845,Standard instruct

# TINYLLAMA MODELS
TinyLlama,zrowt/results,Generic TRL / SFT / Unknown,Category 1: TRL/Generic SFT,0.0574,Unknown/TRL
TinyLlama,lleticiasilvaa/TinyLlama-text2SQL-schemaReduzido,TRL,Category 1: TRL/Generic SFT,0.0626,TRL SQL
TinyLlama,lleticiasilvaa/TinyLlama-1.1B-GerarSQL-v2-identacao,TRL,Category 1: TRL/Generic SFT,0.0627,TRL SQL
TinyLlama,lleticiasilvaa/TinyLlama-1.1B-GerarSQL-v1,TRL,Category 1: TRL/Generic SFT,0.064,TRL SQL
TinyLlama,lleticiasilvaa/TinyLlama-text2SQL-alias-indentacao-ourschema,TRL,Category 1: TRL/Generic SFT,0.0671,TRL SQL
TinyLlama,lleticiasilvaa/TinyLlama-text2SQL-schemaReduzidoTabelas,TRL,Category 1: TRL/Generic SFT,0.0714,TRL SQL
TinyLlama,qbitmaze/ibrain_0002q4,TRL,Category 1: TRL/Generic SFT,0.1598,TRL

TinyLlama,RedHatAI/TinyLlama-1.1B-Chat-v1.0-pruned2.4,Hybrid / Compression（Pruning / SparseGPT）,Category 2: Architecture/Vocab Changes,0.2323,Pruned model

TinyLlama,HachiML/TinyLlama2-jp-122M,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0024,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-5,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0042,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-9,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0084,Japanese + size change
TinyLlama,HachiML/TinyLlama2-jp-122M-FlashAttention2,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0085,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-6,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0125,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-3,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0254,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-1,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0409,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-4,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0427,Japanese + size change
TinyLlama,HachiML/Bit-Llama2-jp-122M-test-1,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0453,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-2,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.051,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-7,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0512,Japanese + size change
TinyLlama,HachiML/myBit-Llama2-jp-127M-test-8,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0914,Japanese + size change
TinyLlama,HachiML/Bit-Llama2-jp-123M-test-1,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.0974,Japanese + size change
TinyLlama,HachiML/Llama2-jp-123M,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.1009,Japanese + size change
TinyLlama,HachiML/Bit-Llama2-jp-123M,Specific Domain / Language / Task SFT,Category 7: Cross-lingual Adaptation,0.1379,Japanese + size change

TinyLlama,ali77sina/tinyLlama-SEC-RAG-FT,Specific Domain / Language / Task SFT,Category 5: Heavy Domain Specialization,0.1146,SEC/Finance domain
TinyLlama,yzhuang/TinyLlama-1.1B_fictional,Specific Domain / Language / Task SFT,Category 5: Heavy Domain Specialization,0.1216,Fictional content
TinyLlama,yzhuang/phi-1_5_fictional,Specific Domain / Language / Task SFT,Category 5: Heavy Domain Specialization,0.173,Fictional content + architecture mismatch?
TinyLlama,ManthanKulakarni/TinyLlama-1.1B-Text2SQL,Specific Domain / Language / Task SFT,Category 5: Heavy Domain Specialization,0.1829,Text2SQL
TinyLlama,yzhuang/TinyLlama-1.1B_fictional_v3,Specific Domain / Language / Task SFT,Category 5: Heavy Domain Specialization,0.2823,Fictional content
