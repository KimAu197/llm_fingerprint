{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08827fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fingerprint_tools.py\n",
    "# ------------------------------------------------------------\n",
    "# Minimal, tidy utilities for RoFL-style x' init + greedy y,\n",
    "# and black-box verification with simple metrics.\n",
    "# NO system prompt anywhere in this file.\n",
    "# You provide:\n",
    "#   - (base) model, tokenizer  —— 用于生成指纹 (x', y)\n",
    "#   - suspect_generate()        —— 被测模型的生成回调（HF 或 GGUF）\n",
    "# ------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "import os, time, json, random, difflib\n",
    "from typing import Callable, List, Dict, Any, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =============== basics ===============\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    import numpy as _np\n",
    "    random.seed(seed)\n",
    "    _np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ts_path(path: str, model_name) -> str:\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    base, ext = os.path.splitext(path)\n",
    "    return f\"{base}_{model_name}{ext or '.json'}\"\n",
    "\n",
    "# =============== formatting (NO system) ===============\n",
    "\n",
    "def format_full_prompt(\n",
    "    x_prime_text: str,\n",
    "    prompt_style: str = \"oneshot\",   # 'oneshot' | 'chatml' | 'raw'\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    - oneshot:\n",
    "        \"user: {x'}\\nassistant:\"\n",
    "    - chatml (Qwen-like, NO system block):\n",
    "        \"<|im_start|>user\\n{x'}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    - raw:\n",
    "        \"{x'}\"\n",
    "    \"\"\"\n",
    "    if prompt_style == \"chatml\":\n",
    "        return (\n",
    "            \"<|im_start|>user\\n\" + x_prime_text + \"\\n<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "    if prompt_style == \"raw\":\n",
    "        return x_prime_text\n",
    "    # default: oneshot\n",
    "    return f\"user: {x_prime_text}\\nassistant:\"\n",
    "\n",
    "# =============== x' initialization (RoFL Step 1 spirit) ===============\n",
    "\n",
    "def _build_allowed_token_set(tokenizer) -> List[int]:\n",
    "    disallow = set()\n",
    "    for attr in [\"bos_token_id\", \"eos_token_id\", \"pad_token_id\", \"unk_token_id\"]:\n",
    "        tid = getattr(tokenizer, attr, None)\n",
    "        if tid is not None:\n",
    "            disallow.add(tid)\n",
    "    if hasattr(tokenizer, \"all_special_ids\"):\n",
    "        disallow.update(tokenizer.all_special_ids)\n",
    "    return [tid for tid in range(tokenizer.vocab_size) if tid not in disallow]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_fingerprint_prompt(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: Optional[str] = None,\n",
    "    l_random_prefix: int = 8,\n",
    "    total_len: int = 64,\n",
    "    k_bottom: int = 50,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    RoFL Step 1 (简化实现):\n",
    "      (1) 前 l 个 token 从 vocab(去掉 special) 均匀随机\n",
    "      (2) 之后每步从“概率最低的 k 个 token”里均匀取一个，直到 total_len\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    allowed = _build_allowed_token_set(tokenizer)\n",
    "    allowed_t = torch.tensor(allowed, device=device)\n",
    "\n",
    "    # (1) 均匀随机前缀\n",
    "    prefix_ids = random.choices(allowed, k=l_random_prefix)\n",
    "    prompt_ids = torch.tensor(prefix_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # (2) bottom-k 扩展\n",
    "    while prompt_ids.shape[1] < total_len:\n",
    "        logits = model(prompt_ids).logits[:, -1, :]\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "        masked = probs.clone()\n",
    "        if len(allowed) < tokenizer.vocab_size:\n",
    "            mask = torch.ones_like(masked, dtype=torch.bool)\n",
    "            mask[:, allowed_t] = False\n",
    "            masked[mask] = 1e9  # 不让不允许 token 落入 bottom-k\n",
    "\n",
    "        _, sorted_idx = torch.sort(masked, dim=-1, descending=False)  # 概率升序\n",
    "        k_eff = min(k_bottom, sorted_idx.shape[1])\n",
    "        bottomk_idx = sorted_idx[:, :k_eff]\n",
    "        next_id = bottomk_idx[0, random.randrange(k_eff)].view(1, 1)\n",
    "        prompt_ids = torch.cat([prompt_ids, next_id.to(device)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(prompt_ids.squeeze(0).cpu(), skip_special_tokens=True)\n",
    "\n",
    "# =============== greedy y (RoFL Step 2) ===============\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_response_hf(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    full_prompt_text: str,\n",
    "    device: Optional[str] = None,\n",
    "    max_new_tokens: int = 64,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    确定性生成（do_sample=False）。返回 continuation（不含输入）。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(full_prompt_text, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,                 # 温度=0\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )[0]\n",
    "    return tokenizer.decode(out_ids[input_len:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# =============== batch: (x', y) generation ===============\n",
    "\n",
    "def generate_fingerprints_batch(\n",
    "    model,\n",
    "    model_name,# 已加载好的 base HF model\n",
    "    tokenizer,                   # 对应 tokenizer\n",
    "    num_pairs: int = 3,\n",
    "    prompt_style: str = \"oneshot\",      # 'oneshot' | 'chatml' | 'raw'\n",
    "    l_random_prefix: int = 8,\n",
    "    total_len: int = 64,\n",
    "    k_bottom: int = 50,\n",
    "    max_new_tokens: int = 64,\n",
    "    save_json_path: Optional[str] = \"fingerprints_init.json\",\n",
    ") -> Tuple[List[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    生成 num_pairs 个 (x', y) 并（可选）保存。\n",
    "    不涉及任何 system 文本。\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        print(f\"[gen] [{i+1}/{num_pairs}]\")\n",
    "        x_prime = sample_fingerprint_prompt(\n",
    "            model, tokenizer, device=device,\n",
    "            l_random_prefix=l_random_prefix, total_len=total_len, k_bottom=k_bottom\n",
    "        )\n",
    "        full_prompt = format_full_prompt(x_prime, prompt_style=prompt_style)\n",
    "        y_resp = greedy_response_hf(model, tokenizer, full_prompt, device=device, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        print(\"x':\", x_prime[:160].replace(\"\\n\", \"\\\\n\"))\n",
    "        print(\"y :\", y_resp[:160].replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "        pairs.append({\n",
    "            \"prompt_style\": prompt_style,\n",
    "            \"x_prime\": x_prime,\n",
    "            \"y_response\": y_resp,\n",
    "            \"full_prompt_used\": full_prompt,\n",
    "        })\n",
    "\n",
    "    out_path = None\n",
    "    if save_json_path:\n",
    "        out_path = ts_path(save_json_path, model_name)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[save] fingerprints -> {out_path}\")\n",
    "\n",
    "    return pairs, out_path\n",
    "\n",
    "# =============== metrics ===============\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "def metric_prefix_match(a: str, b: str, min_len: int = 30) -> int:\n",
    "    return int(_normalize_text(a)[:min_len] == _normalize_text(b)[:min_len])\n",
    "\n",
    "def metric_lcs_ratio(a: str, b: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, _normalize_text(a), _normalize_text(b)).ratio()\n",
    "\n",
    "def metric_signature_overlap(a: str, b: str, min_tok_len: int = 6) -> Tuple[int, int]:\n",
    "    a_norm, b_norm = _normalize_text(a), _normalize_text(b)\n",
    "    toks = [t for t in a_norm.split() if len(t) >= min_tok_len]\n",
    "    hits = sum(1 for t in toks if t in b_norm)\n",
    "    return hits, len(toks)\n",
    "\n",
    "\n",
    "# ================== suspect wrappers (HF / GGUF) ==================\n",
    "\n",
    "class SuspectModelHF:\n",
    "    \"\"\"\n",
    "    Wrapper for a HuggingFace causal LM (transformers).\n",
    "    We'll greedy-generate continuation from full_prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device=None, torch_dtype=torch.float16):\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        self.tok = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        if self.tok.pad_token is None:\n",
    "            self.tok.pad_token = self.tok.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map={\"\": device},   # put whole model on single device\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_answer(self, full_prompt, max_new_tokens=128, stop_tokens=None):\n",
    "        \"\"\"\n",
    "        full_prompt: already includes role markers if any (NO system prompt anywhere).\n",
    "        returns: continuation only (greedy, do_sample=False).\n",
    "        \"\"\"\n",
    "        inputs = self.tok(full_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        out_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,  # greedy == temperature 0\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tok.eos_token_id,\n",
    "            eos_token_id=self.tok.eos_token_id,\n",
    "        )[0]\n",
    "\n",
    "        new_tokens = out_ids[input_len:]\n",
    "        text = self.tok.decode(\n",
    "            new_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "\n",
    "        # optional manual stopping on phrases\n",
    "        if stop_tokens:\n",
    "            for st in stop_tokens:\n",
    "                cut_idx = text.find(st)\n",
    "                if cut_idx != -1:\n",
    "                    text = text[:cut_idx]\n",
    "        return text\n",
    "\n",
    "\n",
    "class SuspectModelLlamaCpp:\n",
    "    \"\"\"\n",
    "    Wrapper for llama_cpp.Llama GGUF model.\n",
    "    Assumes you already created: llm = Llama(...)\n",
    "    \"\"\"\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate_answer(self, full_prompt, max_new_tokens=128, stop_tokens=None):\n",
    "        if stop_tokens is None:\n",
    "            # defaults that work for oneshot/chatml/raw\n",
    "            stop_tokens = [\"</s>\", \"user:\", \"assistant:\", \"<|im_end|>\", \"<|im_start|>user\"]\n",
    "        out = self.llm(\n",
    "            full_prompt,\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=0.0,  # greedy-ish\n",
    "            stop=stop_tokens,\n",
    "        )\n",
    "        return out[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "# ================== evaluation (no system prompt anywhere) ==================\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_fingerprints(\n",
    "    pairs_json_path: str,\n",
    "    model_name: str,\n",
    "    suspect_base_name, # 用于文件名\n",
    "    suspect_model,                  # 有 .generate_answer(full_prompt, max_new_tokens, stop_tokens)\n",
    "    suspect_label: str = \"suspect\",\n",
    "    save_report_path: str | None = None,   # 如 \"eval_report.json\"\n",
    "    min_prefix_len: int = 30,\n",
    "    sig_min_tok_len: int = 6,\n",
    "    use_timestamp_in_name: bool = False,   # 文件名是否带时间戳\n",
    "):\n",
    "    \"\"\"\n",
    "    保存的 summary 只包含: fingerprint, base_y, suspect_y （逐样本列表）。\n",
    "    其他指标只打印，不写入文件。\n",
    "    \"\"\"\n",
    "    # 1) load pairs\n",
    "    with open(pairs_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "    print(f\"[info] Loaded {len(pairs)} fingerprint pairs from {pairs_json_path}\")\n",
    "\n",
    "    # 打印用指标（不保存）\n",
    "    prefix_hits, sim_scores, sig_hits, sig_totals = [], [], [], []\n",
    "\n",
    "    # 读入 pairs 后（evaluate_fingerprints 里）：\n",
    "    style = pairs[0].get(\"prompt_style\", \"raw\")\n",
    "\n",
    "    if style == \"raw\":\n",
    "        stops = [\"</s>\", \"<|im_end|>\"]           # 只保留真正的特殊终止\n",
    "    elif style == \"chatml\":\n",
    "        stops = [\"</s>\", \"<|im_end|>\", \"<|im_start|>user\"]\n",
    "    else:  # oneshot\n",
    "        stops = [\"</s>\", \"user:\", \"assistant:\"]\n",
    "    # 最终要写入文件的极简列表\n",
    "    minimal_records = []\n",
    "\n",
    "    # 2) eval loop\n",
    "    for idx, pair in enumerate(tqdm(pairs, desc=f\"Evaluating on {suspect_label}\")):\n",
    "        x_prime = pair[\"x_prime\"]              # fingerprint\n",
    "        base_y  = pair[\"y_response\"]           # base model 的 y\n",
    "        # 优先使用生成 y 时精确的 full prompt（无 system/有 role，都按你生成时的格式）\n",
    "        if \"full_prompt_used\" in pair:\n",
    "            full_prompt_for_suspect = pair[\"full_prompt_used\"]\n",
    "        else:\n",
    "            # 兜底（无 system、最小 role）\n",
    "            full_prompt_for_suspect = f\"user: {x_prime}\\nassistant:\"\n",
    "\n",
    "        suspect_y = suspect_model.generate_answer(\n",
    "            full_prompt_for_suspect,\n",
    "            max_new_tokens=128,\n",
    "            stop_tokens=stops,\n",
    "        )\n",
    "\n",
    "        # —— 只打印，不保存 ——\n",
    "        pm  = metric_prefix_match(base_y, suspect_y, min_len=min_prefix_len)\n",
    "        sim = metric_lcs_ratio(base_y, suspect_y)\n",
    "        h, tot = metric_signature_overlap(base_y, suspect_y, min_tok_len=sig_min_tok_len)\n",
    "        prefix_hits.append(pm); sim_scores.append(sim); sig_hits.append(h); sig_totals.append(tot)\n",
    "\n",
    "        # —— 保存的极简条目 ——\n",
    "        minimal_records.append({\n",
    "            \"fingerprint\": x_prime,\n",
    "            \"base_y\": base_y,\n",
    "            \"suspect_y\": suspect_y,\n",
    "        })\n",
    "\n",
    "    # 3) 打印整体指标（不写入文件）\n",
    "    prefix_match_rate = float(np.mean(prefix_hits)) if prefix_hits else 0.0\n",
    "    avg_edit_sim      = float(np.mean(sim_scores)) if sim_scores else 0.0\n",
    "    sig_overlap_rate  = (\n",
    "        float(np.sum(sig_hits)) / max(1, float(np.sum(sig_totals)))\n",
    "        if np.sum(sig_totals) > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    print(\"========== FINGERPRINT VERIFICATION REPORT ==========\")\n",
    "    print(f\"Suspect model label: {suspect_label}\")\n",
    "    print(f\"#pairs evaluated: {len(pairs)}\")\n",
    "    print(f\"Prefix match rate (first {min_prefix_len} chars): {prefix_match_rate:.3f}\")\n",
    "    print(f\"Avg edit/sequence similarity (0~1):           {avg_edit_sim:.3f}\")\n",
    "    print(f\"Signature phrase overlap (>= {sig_min_tok_len} chars): {sig_overlap_rate:.3f}\")\n",
    "    print(\"Preview (first 2 minimal records):\")\n",
    "    for rec in minimal_records[:2]:\n",
    "        print(\"----\")\n",
    "        print(\"fingerprint:\", rec[\"fingerprint\"][:160].replace(\"\\n\",\"\\\\n\"))\n",
    "        print(\"base_y     :\", rec[\"base_y\"][:160].replace(\"\\n\",\"\\\\n\"))\n",
    "        print(\"suspect_y  :\", rec[\"suspect_y\"][:160].replace(\"\\n\",\"\\\\n\"))\n",
    "\n",
    "    out_obj = {\n",
    "        \"base_model_name\": model_name,\n",
    "        \"suspect_model_name\": suspect_base_name,\n",
    "        \"prompt_style\": style,\n",
    "        \"num_pairs\": len(pairs),\n",
    "        \"records\": minimal_records,\n",
    "    }\n",
    "\n",
    "    # 4) 保存：仅极简列表\n",
    "    out_path = None\n",
    "    if save_report_path:\n",
    "        out_path = ts_path(save_report_path, model_name=model_name)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out_obj, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[save] minimal summary -> {out_path}\")\n",
    "\n",
    "    return minimal_records, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SuspectFromLoadedHF:\n",
    "    def __init__(self, model, tok):\n",
    "        self.model = model.eval()\n",
    "        self.tok = tok\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_answer(self, full_prompt, max_new_tokens=128, stop_tokens=None):\n",
    "        inputs = self.tok(full_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        inp_len = inputs[\"input_ids\"].shape[1]\n",
    "        out_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,  # greedy (= temp 0)\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tok.eos_token_id,\n",
    "            eos_token_id=self.tok.eos_token_id,\n",
    "        )[0]\n",
    "        text = self.tok.decode(out_ids[inp_len:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        if stop_tokens:\n",
    "            for st in stop_tokens:\n",
    "                i = text.find(st)\n",
    "                if i != -1:\n",
    "                    text = text[:i]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# def load_hf_model(model_name: str, suspect_model: str, device: str | None = None, dtype: torch.dtype | None = None):\n",
    "#     \"\"\"\n",
    "#     Universal Hugging Face model loader.\n",
    "#     Returns (model, tokenizer, device).\n",
    "#     - Handles pad_token and left-padding consistently.\n",
    "#     - Defaults to float16 (for T4 / Colab / GPU efficiency).\n",
    "#     \"\"\"\n",
    "#     if device is None:\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     if dtype is None:\n",
    "#         dtype = torch.float16\n",
    "\n",
    "#     # --- Tokenizer Loading ---\n",
    "#     tok = None\n",
    "#     try:\n",
    "#         tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[warn] tokenizer for {model_name} failed: {e}\")\n",
    "#         try:\n",
    "#             print(f\"[fallback] trying {suspect_model} instead...\")\n",
    "#             tok = AutoTokenizer.from_pretrained(suspect_model, trust_remote_code=True, use_fast=False)\n",
    "#         except Exception as e2:\n",
    "#             raise RuntimeError(f\"Failed to load both tokenizers ({model_name}, {suspect_model}): {e2}\")\n",
    "\n",
    "#     if not hasattr(tok, \"pad_token\"):   # <-- safety guard\n",
    "#         raise TypeError(f\"Tokenizer load failed. Got type: {type(tok)}\")\n",
    "\n",
    "#     # Ensure pad_token exists\n",
    "#     if tok.pad_token is None:\n",
    "#         tok.pad_token = tok.eos_token or tok.unk_token\n",
    "#     tok.padding_side = \"left\"\n",
    "\n",
    "#     # --- Model Loading ---\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         torch_dtype=dtype,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True,\n",
    "#         device_map={\"\": device},\n",
    "#     ).eval()\n",
    "\n",
    "#     return model, tok, device\n",
    "\n",
    "def load_hf_model(model_id, fourbit=False, torch_dtype=torch.float16, device_map=\"auto\"):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = dict(trust_remote_code=True, device_map=device_map)\n",
    "    if fourbit:\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        kwargs[\"quantization_config\"] = bnb\n",
    "    else:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
    "    model.eval()\n",
    "    return model, tok, next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === memory_cleanup.py（或直接放到一个cell里）===\n",
    "import gc, torch\n",
    "\n",
    "def unload_hf_model(model=None, tokenizer=None):\n",
    "    \"\"\"释放 HF 模型与显存/内存。兼容 device_map('auto') / 单卡。\"\"\"\n",
    "    try:\n",
    "        if model is not None:\n",
    "            try:\n",
    "                model.to('cpu')  # 先转回 CPU，避免有残留显存句柄\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if tokenizer is not None:\n",
    "            del tokenizer\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Python 对象回收\n",
    "    gc.collect()\n",
    "\n",
    "    # CUDA 显存清理（含跨进程共享块）\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def unload_llama_cpp(llm=None):\n",
    "    \"\"\"释放 llama.cpp GGUF 实例（如用到的话）。\"\"\"\n",
    "    try:\n",
    "        if llm is not None and hasattr(llm, \"close\"):\n",
    "            llm.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del llm\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fccb18",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cognitivecomputations/dolphin-2.9.3-mistral-7B-32k\"\n",
    "model_orgin = \"dolphin-2.9.3-mistral-7B-32k\"\n",
    "model_base_name = \"mistralai/Mistral-7B-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5764334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- model1: 基础模型（用来生成指纹） --------\n",
    "model1, tok1, dev1 = load_hf_model(model_name, fourbit=False)\n",
    "\n",
    "# -------- model2: 被测模型（在指纹上验证） --------\n",
    "model2, tok2, dev2 = load_hf_model(model_base_name, fourbit=False)\n",
    "print(\"Loaded:\\n - model1 on\", dev1, \"\\n - model2 on\", dev2)\n",
    "\n",
    "# 可选：快速贪心自测（不含 system，保持确定性）\n",
    "@torch.no_grad()\n",
    "def greedy_once(model, tok, prompt, max_new_tokens=50):\n",
    "    device = next(model.parameters()).device\n",
    "    inp = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(\n",
    "        **inp,\n",
    "        do_sample=False,               # = temperature 0\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "    )[0]\n",
    "    print(tok.decode(out[inp[\"input_ids\"].shape[1]:], skip_special_tokens=True))\n",
    "\n",
    "# 示例（可删）：不使用 system，仅 oneshot 结构\n",
    "test_prompt = \"user: Hi, introduce your self \\nassistant:\"\n",
    "greedy_once(model1, tok1, test_prompt, 40)\n",
    "greedy_once(model2, tok2, test_prompt, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fingerprint_tools import set_seed, generate_fingerprints_batch\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "pairs, pairs_path = generate_fingerprints_batch(\n",
    "    model=model1,\n",
    "    model_name=model_orgin,\n",
    "    tokenizer=tok1,\n",
    "    num_pairs=3,                 #  3 个\n",
    "    prompt_style=\"raw\",      # 只用 \"user: ...\\nassistant:\" 这种最小格式\n",
    "    # system_prompt=\"\",            # 不要 system（传空字符串）\n",
    "    l_random_prefix=8,\n",
    "    total_len=64,\n",
    "    k_bottom=50,\n",
    "    max_new_tokens=64,\n",
    "    save_json_path=\"fingerprints_init.json\",  # 会自动带时间戳落盘\n",
    ")\n",
    "\n",
    "print(\"Saved to:\", pairs_path)\n",
    "print(\"x'[0]:\", pairs[0][\"x_prime\"][:120])\n",
    "print(\"y [0]:\", pairs[0][\"y_response\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fingerprint_tools import evaluate_fingerprints\n",
    "\n",
    "suspect = SuspectFromLoadedHF(model2, tok2)\n",
    "\n",
    "report = evaluate_fingerprints(\n",
    "    pairs_json_path=pairs_path,\n",
    "    model_name = model_orgin,\n",
    "    suspect_base_name=model_base_name,\n",
    "    suspect_model=suspect,\n",
    "    suspect_label=\"model2-on-model1-fp\",\n",
    "    save_report_path=\"eval_report.json\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
